# example configuration file for ceph-bluestore.fio

[global]
	debug bluestore = 0/0
	debug bluefs = 0/0
	debug bdev = 0/0
	debug rocksdb = 0/0
	# spread objects over 8 collections
	osd pool default pg num = 1
	# increasing shards can help when scaling number of collections
	osd op num shards = 5
	bluestore block create = true

	bluestore block path = /dev/nvme0n1p6
	bluestore min alloc size =4096
#	bluestore bluefs = false
#	bluestore debug omit block device write = true
#	bluestore debug omit kv commit = true
#	bluefs_sync_write = false
	bluestore fsck on mkfs =false
#	rocksdb perf = true
#	bluestore debug no reuse blocks = true
	bluestore rocksdb options = compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=2097152

#,memtable=hash_linkedlist,allow_concurrent_memtable_write=false

[osd]
	osd objectstore = bluestore

	# use directory= option from fio job file
	osd data = ${fio_dir}

	# log inside fio_dir
	log file = ${fio_dir}/log

